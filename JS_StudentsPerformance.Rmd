---
title: 'Prediction of Secondary School Student Grades Using UCI Data '
author: "Jeong Sukchan"
date: "11/21/2021"
output:
  html_document: default
---
	<Structure of the Report>
	1. Introduction
	   Abstract 
	   1.1. Problem Identification
	   1.2. Goal 
	   1.3. Key Steps   
	   1.4. Dataset
	        1.4.1. Installing necessary packages
	        1.4.2. Data collection 
	        1.4.3. Dataset Information
	2. Method and Analysis
	   2.1. Data Preparation
	   2.2. EDA(Exploratory Data Analysis)
	        2.2.1. Overview of the Dataset
	        2.2.2. Data Analysis and Visualization
	   2.3.  Data Pre-processing_Data Modification and Cleaning
	        2.3.1. Checking NA (Null Values)
	        2.3.2. Checking Outliers
	        2.3.3. Data Partition
	3. Modelling
	   3.1. Model 1: Guessing Model
	   3.2. Model 2: Logistic regression Model
	   3.3. Model 3: Simplified Logistic Regression with Significant Variables
	   3.4. Model 4: Cross-validated Decision Tree
	   3.5. Model 5: Cross-validated KNN Model
	   3.6. Model 6: Random Forest Model
	4. Results
	5. Conclusion
-------------

## 1. Introduction
#### Summary
In this paper, we analyze a dataset at UCI website. We collect a data named "student-mat" and get information from that website. The goal of this paper is to predict students' final performance result of either pass or fail. The reason for this goal is that educators might help academically poor students who has higher probability to fail in advance. To achieve the goal, the aim of this paper is to build a good model with high accuracy result for binary classification data approaching some machine learning algorithms. 

In the section 2(Method and Analysis), we overview the dataset and analyze relation among variables. From data analysis we find out directly influential variables that affect the result of the dependent variable. These are sex, age, address, famsize, Medu, Fedu, Mjob, Fjob, traveltime, studytime, failures, schoolsup, paid, higher, romantic, goout, G1 and G2. They are directly correlated with G3 compared to other variables. In addition, we find out indirectly influential variables. These are famsup, school, internet, Dalc, Walc, freetime and gauardian variables. They are correlated with the influential variables that is correlated with G3 directly. Therefore, excluded variables are  Pstatus, reason, famsup, activities, nursery, famrel, health and absences variables that are less correlated with the dependent variable directly and indirectly. Before builing up several algorithms, we modify our dataset by checking NA and outliers; by splitting it into trainset and testset.  

In the section 3(Modelling), We approach some algorithms such as Guessing, Regression, Decision Trees, KNN and Random Forest method training each model with trainset and predicting with testset. The models are evaluated by accuracy instrument. In the section 4(Results), the result table is shown on which we select the best optimal model. In the section 5(Conclusion), we conclude by presenting summary, potential impact, limitation of the report and future work. 

### 1.1. Problem Identification
It is no doubt that students and educators have in common regarding getting a good grade. Especially, educators may wonder how to help out students who have the high probability to fail the exam in advance. Then, which factors are critical to predict their grades? Which model would tell their final grade well before taking the final exam?
 
### 1.2. Goal 
The goal is to predict students' final performance result of either pass or fail. To achieve the goal, the aim of this paper is to build a good model with a high accuracy results for binary data. 

### 1.3. Key Steps
We will go through  the 4 steps.<br>
- Data import and preparation <br>
- Data Analysis and data pre-process <br>
- Modelling <br>
- Evaluation<br>  

### 1.4. Dataset
The reason to choose our dataset is the accessibility which we can get from UCI Machine Learning Repository easily. 

#### 1.4.1. Installing necessary packages 
Before we start importing our dataset, we will install the necessary packages that we need for analysis and modelling. 
```{r}
# Create package to need for our report 
pkg<-c("tidyverse", "dslabs","dplyr","ggplot2","caret","data.table","lattice","readr","magrittr","skimr","Hmisc","psych","gridExtra","doBy", "corrplot", "pheatmap", "ROCR", "gplots", "irr", "janitor", "PerformanceAnalytics", "rpart", "rattle")

# Create package to need for installation
new_pkg<-pkg[!(pkg %in% rownames(installed.packages()))]

# if there are packages uninstalled, install the package at a time
if(length(new_pkg))install.packages(new_pkg, dependencies = TRUE)

# Road the package at a time
suppressMessages(sapply(pkg, require, character.only=TRUE)) # suppress the complicated messages.
```

#### 1.4.2. Data collection
The dataset were imported from UCI Machine Learning Respository website: 
http://archive.ics.uci.edu/ml/datasets/Student+Performance#\n

The file is located at: 
http://archive.ics.uci.edu/ml/machine-learning-databases/00320/student.zip

The file name is "student-mat.csv"
```{r}
# Download the dataset in our local computer and load the data 

# An example of the path of downloaded and unzipped folder in a local computer: "C:/datascience/r/projects/performance/data/student/"

# Load the data and store "mat" objective
mat <- read.table("C:/datascience/r/projects/performance/data/student/student-mat.csv", sep=";",header=TRUE)
```

#### 1.4.3. Dataset Information
The dataset information is at the website where our dataset was downloaded. 
The dataset has 395 observations and 33 variables. 

#### Attribute Information
1 school - student's school (binary: 'GP' - Gabriel Pereira or 'MS' - Mousinho da Silveira) <br>
2 sex - student's sex (binary: 'F' - female or 'M' - male) <br>
3 age - student's age (numeric: from 15 to 22) <br>
4 address - student's home address type (binary: 'U' - urban or 'R' - rural) <br>
5 famsize - family size (binary: 'LE3' - less or equal to 3 or 'GT3' - greater than 3)<br>
6 Pstatus - parent's cohabitation status (binary: 'T' - living together or 'A' - apart)<br>
7 Medu - mother's education (numeric: 0 - none, 1 - primary education (4th grade), 2 - 5th to 9th grade, 3 - secondary education or 4 - higher education)<br>
8 Fedu - father's education (numeric: 0 - none, 1 - primary education (4th grade), 2 - 5th to 9th grade, 3 - secondary education or 4 - higher education)<br>
9 Mjob - mother's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')<br>
10 Fjob - father's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')<br>
11 reason - reason to choose this school (nominal: close to 'home', school 'reputation', 'course' preference or 'other')<br>
12 guardian - student's guardian (nominal: 'mother', 'father' or 'other')<br>
13 traveltime - home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)<br>
14 studytime - weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)<br>
15 failures - number of past class failures (numeric: n if 0<=n<3, else 4)<br>
16 schoolsup - extra educational support (binary: yes or no)<br>
17 famsup - family educational support (binary: yes or no)<br>
18 paid - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)<br>
19 activities - extra-curricular activities (binary: yes or no)<br>
20 nursery - attended nursery school (binary: yes or no)<br>
21 higher - wants to take higher education (binary: yes or no)<br>
22 internet - Internet access at home (binary: yes or no)<br>
23 romantic - with a romantic relationship (binary: yes or no)<br>
24 famrel - quality of family relationships (numeric: from 1 - very bad to 5 - excellent)<br>
25 freetime - free time after school (numeric: from 1 - very low to 5 - very high)<br>
26 goout - going out with friends (numeric: from 1 - very low to 5 - very high)<br>
27 Dalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high)<br>
28 Walc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)<br>
29 health - current health status (numeric: from 1 - very bad to 5 - very good)<br>
30 absences - number of school absences (numeric: from 0 to 93)<br>
31 G1 - first period grade (numeric: from 0 to 20)<br>
31 G2 - second period grade (numeric: from 0 to 20)<br>
32 G3 - final grade (numeric: from 0 to 20, output target)

No missing value

#### Dependent variable
Among the variables, G3 will be the dependent variable which is a final score after G1 and G2 evaluations. The grading point scale is 20 in which 0 is the lowest and 20 is the perfect. 

#### Data Type
Attributes are either character(binary or nominal) or numeric.<br>
- Binary character: school, sex, address, famsize, Pstatus, schoolsup, famsup, paid, activities, nursery, higher, internet, romantic<br>
- Numeric variables: age, Medu, Fedu, traveltime, studytime, failures, famrel, freetime, goout, Dalc, Walc, health, absences, G1, G2, G3<br>
- Nominal character (with five levels): Mjob, Fjob, reason, guardian


## 2. Method and Analysis
In this section, the process and technique are the following.<br>
  - Data preparation: We will modify our data into appropriate several forms so that they can be applied to specific algorithms.<br>
  - Overviewing of the data: We will try to get a whole picture of properties and relations of variables.<br>
  - Data pre-processing: We will check Null Values (NA) and outliers, and split datasets into trainsets and testsets before modelling. 

### 2.1. Data Preparation

The "mat" dataset is ready for analysis. But we will modify it for specific uses by creating several datasets. We will use them accordingly in EDA and modelling.  Further we will clean and modify more in the "Data Pre-processing" section and other sections as necessary. 

```{r}
# Create a new dataset called "new_mat" for changing numeric G1, G2 and G3 variables into binary variables. Let's define G3>= 8 as "1" meaning "pass" and otherwise, "0" meaning "fail". The new_mat dataset has a binary dependant variable whereas the mat dataset has a numeric dependant variable. We will use both accordingly.  
new_mat<-mat
new_mat$G3<-ifelse(new_mat$G3>=8, 1, 0)
new_mat$G1<-ifelse(new_mat$G1>=8, 1, 0)
new_mat$G2<-ifelse(new_mat$G2>=8, 1, 0)

# Create a factor dataset called "fnew_mat". 
fnew_mat<-new_mat
fnew_mat$school<-factor(mat$school, levels=c("GP","MS"))
fnew_mat$sex <- factor(mat$sex, levels=c("F","M"))
fnew_mat$age <- factor(mat$age, levels=c("15","16", "17","18","19","20","21","22"))
fnew_mat$address <- factor(mat$address, levels=c("U","R"))
fnew_mat$famsize <- factor(mat$famsize, levels=c("LE3","GT3"))
fnew_mat$Pstatus <- factor(mat$Pstatus, levels=c("T","A"))
fnew_mat$Medu <- factor(mat$Medu, levels=c("0","1", "2","3","4")) 
fnew_mat$Fedu <- factor(mat$Fedu, levels=c("0","1", "2","3","4")) 
fnew_mat$Mjob <- factor(mat$Mjob, levels=c("teacher","health","services","at_home","other"))
fnew_mat$Fjob <- factor(mat$Fjob, levels=c("teacher","health","services","at_home","other"))
fnew_mat$reason <- factor(mat$reason, levels=c("home","reputation","course","other"))
fnew_mat$guardian <- factor(mat$guardian, levels=c("mother","father","other"))
fnew_mat$traveltime <- factor(mat$traveltime, levels=c("1", "2","3","4")) 
fnew_mat$studytime <- factor(mat$studytime, levels=c("1", "2","3","4")) 
fnew_mat$failures <- factor(mat$failures, levels=c("0","1", "2","3","4")) 
fnew_mat$schoolsup <- factor(mat$schoolsup, levels=c("yes","no"))
fnew_mat$famsup <- factor(mat$famsup, levels=c("yes","no"))
fnew_mat$paid <- factor(mat$paid, levels=c("yes","no"))
fnew_mat$activities <- factor(mat$activities, levels=c("yes","no"))
fnew_mat$nursery <- factor(mat$nursery, levels=c("yes","no"))
fnew_mat$higher <- factor(mat$higher, levels=c("yes","no"))
fnew_mat$internet <- factor(mat$internet, levels=c("yes","no"))
fnew_mat$romantic <- factor(mat$romantic, levels=c("yes","no"))
fnew_mat$famrel <- factor(mat$famrel, levels=c("1", "2","3","4", "5")) 
fnew_mat$freetime <- factor(mat$freetime, levels=c("1", "2","3","4", "5")) 
fnew_mat$goout <- factor(mat$goout, levels=c("1", "2","3","4", "5")) 
fnew_mat$Dalc <- factor(mat$Dalc, levels=c("1", "2","3","4", "5")) 
fnew_mat$Walc <- factor(mat$Walc, levels=c("1", "2","3","4", "5")) 
fnew_mat$health <- factor(mat$health, levels=c("1", "2","3","4", "5")) 
fnew_mat$absences<-ifelse(new_mat$absence<2, 0, 
                          ifelse(new_mat$absences>=3 & new_mat$absences<10, 1, 
                                 ifelse(new_mat$absences >=10 & new_mat$absences <20, 2, 
                                        ifelse(new_mat$absences >=20 & new_mat$absences <40, 3, 4))))
fnew_mat$absences<-factor(new_mat$absences, levels=c("0", "1", "2", "3", "4"))
fnew_mat$G1<-factor(new_mat$G1, levels=c("0", "1"), labels=c("fail", "pass"))
fnew_mat$G2<-factor(new_mat$G2, levels=c("0", "1"), labels=c("fail", "pass"))
fnew_mat$G3<-factor(new_mat$G3, levels=c("0", "1"), labels=c("fail", "pass"))

# Create a binomial dataset where G3 is binomial dependent variable and others are factors.
bi<-fnew_mat
bi$G1<-new_mat$G1
bi$G1<-as.factor(new_mat$G1)
bi$G2<-new_mat$G2
bi$G2<-as.factor(new_mat$G2)
bi$G3<-new_mat$G3
bi$G3<-as.factor(new_mat$G3)

# Create a numeric dataset called "num" for analizing correlation among numeric attributes
num<-dplyr::select(mat, age, Medu, Fedu, traveltime, studytime, failures, famrel, freetime, goout,Dalc, Walc,health,absences, G1, G2, G3)

# Create numeric dataset called "allnum" for all variables
allnum<-mat
allnum$school<-as.numeric(ifelse(fnew_mat$school=="GP", 0, 1)) 
allnum$sex<-as.numeric(ifelse(fnew_mat$sex=="F", 1, 0)) 
allnum$address<-as.numeric(ifelse(fnew_mat$address=="U", 1, 0)) 
allnum$famsize<-as.numeric(ifelse(fnew_mat$famsize=="LE3", 0, 1)) 
allnum$Pstatus<-as.numeric(ifelse(fnew_mat$Pstatus=="T", 1, 0)) 
allnum$Mjob<-as.numeric(ifelse(fnew_mat$Mjob=="teacher", 4, 
                                 ifelse(fnew_mat$Mjob=="health", 3, 
                                        ifelse(fnew_mat$Mjob=="services", 2,
                                               ifelse(fnew_mat$Mjob=="at_home", 1, 0))))) 
allnum$Fjob<-as.numeric(ifelse(fnew_mat$Fjob=="teacher", 4, 
                                 ifelse(fnew_mat$Fjob=="health", 3, 
                                        ifelse(fnew_mat$Fjob=="services", 2,
                                               ifelse(fnew_mat$Fjob=="at_home", 1, 0))))) 
allnum$reason<-as.numeric(ifelse(fnew_mat$reason=="home", 3, 
                                 ifelse(fnew_mat$reason=="reputation", 2, 
                                        ifelse(fnew_mat$reason=="course", 1, 0))))
allnum$guardian<-as.numeric(ifelse(fnew_mat$guardian=="mother", 2, 
                                 ifelse(fnew_mat$guardian=="father", 1, 0))) 
allnum$schoolsup<-as.numeric(ifelse(fnew_mat$schoolsup=="yes", 1, 0)) 
allnum$famsup<-as.numeric(ifelse(fnew_mat$famsup=="yes", 1, 0)) 
allnum$paid<-as.numeric(ifelse(fnew_mat$paid=="yes", 1, 0)) 
allnum$activities<-as.numeric(ifelse(fnew_mat$activities=="yes", 1, 0)) 
allnum$nursery<-as.numeric(ifelse(fnew_mat$nursery=="yes", 1, 0)) 
allnum$higher<-as.numeric(ifelse(fnew_mat$higher=="yes", 1, 0)) 
allnum$internet<-as.numeric(ifelse(fnew_mat$internet=="yes", 1, 0)) 
allnum$romantic<-as.numeric(ifelse(fnew_mat$romantic=="yes", 1, 0)) 

# Create a new dataset called "ex_mat, ex_num, ex_new_mat, ex_fnew_mat" for removing G1 and G2 because of too obviously decisive indicators of the dependent variable and becasue of a wider application in other cases where many schools might not have the results of the previous tests
ex_num<-num[, c(1:13,16)]
ex_new_mat<-new_mat[, c(1:30,33)]
ex_bi<-bi[, c(1:30,33)]
```

### 2.2. EDA (Exploratory Data Analysis)
#### 2.2.1. Overview of the Dataset
Let's take a look an overall picture of the "mat" dataset through statistical analysis and get some insights from it.
We will use different packages for statistical analysis because each package has the unique way with different statistical displays.  

#### a. Statistical analysis  
```{r}
str(mat)
```
There are 395 observations of 33 variables. We can see the content of levels for character variable. 

#### b. Statistical analysis  
```{r}
Hmisc::describe(mat)
```
We can clearly check the number of missing data, frequency and proportion of levels of each variables. We will analyze some variables in the end of this section.   

#### c. Statistical analysis 
```{r}
psych::describe(mat)
```
The table above provides a very comprehensive list of the descriptive statistics for our dataset. 
As we are just going to scroll across the table, it shows us the variable number, "n" which indicates the number of observations for each variable, "mean" which indicates the average value, "standard deviation", "median", "trimmed", "mad" which means the mean absolute deviation, "max", "min" which means the minimum value, "range" which means between the two, "skew" means the measure of how skewed our dataset is, "kurtosis" and "se" which indicates the standard error. 
This statistics are very useful for the numeric variables but not for the categorical ones which are denoted with asterisks in the table.  <br>

#### d. Statistical analysis 
```{r}
skimr::skim(mat)
```
We can quickly assess our data features and types. It goes even deeper by each character variables arranged according to data types. The above two tibbles are grouped by variable types: character and numeric. It gives some information about the number of missing data, completed rate, distribution of the values. It also shows unique values for character variables, whereas it displays distribution such as mean, standard deviation, quantile and a little histogram for numeric variables. 

#### 2.2.2. Data Analysis and Visualization
In this section, we will explore the data by asking and answering questions about variables and their relations to get some insights.  

#### a. Show graphs for all variables.
```{r}
# Let's explore overall tendency of each variable displaying multiple histograms
par(mfrow=c(3,3)) # Show 3*3 graphs on each pane
hist.data.frame(mat)
```

- school: Most of the students live in urban areas.<br>
- sex: There are more girls than boys. Hmisc Statistical Analysis tells us proportion of sex: female 0.527; male 0.473.  <br>
- age: The most of the students are aged between 15 and 18. From the above statistics in the previous section, there are 29 students older than 18 years old.
The average of students' age is 16.7. <br>
- address: Most students live in urban areas rather than in rural areas.<br>
- famsize: More students live with more than 3 family members.<br>
- Pstatus: Most students live with their parents.<br>
- Medu and Fedu: The level of parents' education is widely distributed from primary education to higher education<br>
- Mjob and Fjob: Many parents work in service sections.<br>
- reason: The most reason to choose the school is related to grade such as course and reputation. <br>
- guardian: Most students have guardians. The mojority of students think mother is their guardian.  <br>
- traveltime: Most students live near their house taking less than  15min.<br>
- studytime: Many students study between 2 and 5 hours a week. <br>
- failures: Most students have not experienced failures of past classes. <br>
- schoolsup: Most students have no extra educational support. <br>
- paid: More students have no extra paid classes in Math. <br>
- activities: More students have extra curricular activities. <br>
- nursery: More students attended nursery school. <br>
- higher: The mojority students want to take higher education. <br>
- internet: More students can access internet at home. <br>
- romantic: More students have no romantic relationship.<br>
- famrel: The mojority students think the quality of family relationships are more than good.<br>
- goout: The going out with their friends are distributed in a wide range. <br>
- Dalc: Most students did not drink alcohol during weekday. <br>
- Walc: Many students drink alcohol during weekend. <br>
- health: the majority of the students are in good health. from 1 - very bad to 5 - very good. But quite number of students are in bad health conditions: 47 out of 395 for 1 indicator (very bad), 45 for 2.<br>
- absences: Most students attend school but some students has many school absences. <br>
- G1, G2 and G3: They show graphs close to normal distribution. Surprisingly, there are quite a lot of students failed in the end of the course in G3. <br>

```{r}
par(mfrow=c(1,1)) # Return the format of displaying graph showing 1*1 graph on each pane
```

#### b. How many observations and variables are in the dataset?
```{r}
dim(new_mat)
```
There are 395 observations and 33 variables.  

#### c. What proportion of individuals is passed?
```{r}
mean(new_mat$G3)
```

#### d. What is the name of columns of numeric variabls?
```{r}
names(num) 
```

#### e. Show the distribution of the dependent variable G3
```{r}
mat$G3%>%qplot()
```

```{r}
janitor::tabyl(mat$G3)%>%print # Show frequency and percentage in original G3 variable. 
```
```{r}
tabyl(fnew_mat$G3) # Show frequency and percentage in binomial G3 variable.  
```
There are around 18% students that failed the test. 

#### f. Show analysis tibble for whether there is the difference in the characteristics of the numeric variable for G3.
```{r}
num%>%select(age:absences, G3)%>% group_by(G3)%>%summarise_all(mean) 
```
Medu, Fedu, studytime, failures, goout, Walc and absences are significant variables of which failure and absences are the most. 

#### g. Show box plots analyzing whether there is the difference in the level of numeric variables for G3.
```{r}
library(ggplot2); library(gridExtra)#########################failed 색깔
p1<-ggplot(data=mat, aes(x=G3, y=age, fill=G3)) + geom_boxplot()
p2<-ggplot(data=mat, aes(x=G3, y=Medu, fill=G3)) + geom_boxplot()
p3<-ggplot(data=mat, aes(x=G3, y=Fedu, fill=G3)) + geom_boxplot()
p4<-ggplot(data=mat, aes(x=G3, y=traveltime, fill=G3)) + geom_boxplot()
p5<-ggplot(data=mat, aes(x=G3, y=studytime, fill=G3)) + geom_boxplot()
p6<-ggplot(data=mat, aes(x=G3, y=failures, fill=G3)) + geom_boxplot()
p7<-ggplot(data=mat, aes(x=G3, y=famrel, fill=G3)) + geom_boxplot()
p8<-ggplot(data=mat, aes(x=G3, y=freetime, fill=G3)) + geom_boxplot()
p9<-ggplot(data=mat, aes(x=G3, y=goout, fill=G3)) + geom_boxplot()
p10<-ggplot(data=mat, aes(x=G3, y=Dalc, fill=G3)) + geom_boxplot()
p11<-ggplot(data=mat, aes(x=G3, y=Walc, fill=G3)) + geom_boxplot()
p12<-ggplot(data=mat, aes(x=G3, y=health, fill=G3)) + geom_boxplot()
p13<-ggplot(data=mat, aes(x=G3, y=absences, fill=G3)) + geom_boxplot()
p14<-ggplot(data=mat, aes(x=G3, y=school, fill=G3)) + geom_boxplot()
p15<-ggplot(data=mat, aes(x=G3, y=sex, fill=G3)) + geom_boxplot()
p16<-ggplot(data=mat, aes(x=G3, y=address, fill=G3)) + geom_boxplot()
p17<-ggplot(data=mat, aes(x=G3, y=famsize, fill=G3)) + geom_boxplot()
p18<-ggplot(data=mat, aes(x=G3, y=Pstatus, fill=G3)) + geom_boxplot()
p19<-ggplot(data=mat, aes(x=G3, y=Mjob, fill=G3)) + geom_boxplot()
p20<-ggplot(data=mat, aes(x=G3, y=Fjob, fill=G3)) + geom_boxplot()
p21<-ggplot(data=mat, aes(x=G3, y=guardian, fill=G3)) + geom_boxplot()
p22<-ggplot(data=mat, aes(x=G3, y=schoolsup, fill=G3)) + geom_boxplot()
p23<-ggplot(data=mat, aes(x=G3, y=famsup, fill=G3)) + geom_boxplot()
p24<-ggplot(data=mat, aes(x=G3, y=paid, fill=G3)) + geom_boxplot()
p25<-ggplot(data=mat, aes(x=G3, y=activities, fill=G3)) + geom_boxplot()
p26<-ggplot(data=mat, aes(x=G3, y=nursery, fill=G3)) + geom_boxplot()
p27<-ggplot(data=mat, aes(x=G3, y=higher, fill=G3)) + geom_boxplot()
p28<-ggplot(data=mat, aes(x=G3, y=internet, fill=G3)) + geom_boxplot()
p29<-ggplot(data=mat, aes(x=G3, y=romantic, fill=G3)) + geom_boxplot()
```

```{r}
grid.arrange(p1,p2,p3, p4,p5, p6,p7, p8,p9, p10,p11, p12,p13, ncol=3)

```

Fedu and failures seems significant variables for G3. 

#### h. Show box plots analyzing whether there is the difference in the level of character variables for G3.
```{r}
grid.arrange(p14,p15, p16,p17, p18,p19, p20,p21,p22,p23,p24,p25,p26,p27,p28,p29, ncol=3)
```

school, sex, address, Mjob, Fjob, gaurdian, schoolsup, higher and internet are factors which affect on results of G3. <br>

#### Correlation 
Let's examine and analyse how our variables are correlated with each other.  

#### i. Show correlation numbers in the numeric variables

```{r}
round(cor(allnum),3)
```
The correlation varies between 1(positive correlation) and -1(negative correlation). 
school,Pstatus, reason, guardian, famsup, activities, nursery, internet, famrel, freetime, absences, Dalc and Walc variables are less related with G3 compared to other variables.  

#### j. Visualize the correlations
It is difficult to see the correlation in numbers at once. Let's visualize it. 
```{r}
pheatmap(cor(num))
```

The correlation varies between 1 (positive) and -1 (negative). The dark red color indicates the high correlated positive relation, for example: G1 and G3; G2 and G3. The dark blue color indicates highly correlated negative relation, such as failures and study time. 

```{r}
pheatmap(cor(allnum))
```

#### k. Show a heatmap with correlation numbers in the numeric variables
```{r}
corrplot.mixed(cor(num))
```

- Positive correlation with G3: G2, G1, Medu, Fedu, studytime 
- Negative correlation with G3: failures, age, freetime, goout
- Positive correlation among variables: Dalc and Walc, Medu and Fedu, goout and Walc/Dalc, freetime and goout/ Dalc, age and failures, G2 and G3, G1 and G2, G1 and G3
- Medu, Fedu, studytime, failures, age, freetime, goout seem to be decisive variables. 

```{r}
# Let's think more correlated variables except the above results of correlation from "num" objective
corrplot.mixed(cor(allnum))
```


- Positive correlation with G3 (except the above results): address, Mjob, Fjob, paid, higher
- Negative correlation with G3 (except the above results): sex, famsize, traveltime, schoolsup, romantic
- Positive correlation among variables (1> correlation >=0.2) (except the above results): Medu and Mjob, Fedu and Fjob, Fedu and Mjob, Mjob and Fjob, famsup and paid, school and traveltime, address and internet, G1/G2/G3 and Medu, Medu and internet
- Negative correlation among variables (-1< correlation < - 0.2) (except the above results): failures and G1/G2/G3, failures and higher, failures and Medu/Fedu, failures and guardian, address and traveltime, sex and Dalc/Walc, studytime and Dwal/Walc, sex and freetime, age and guardian/schoolsup/higher 

To sum up about correlated variables

- Fitst, direct influential variables whose absolute value of the relation with G3 is greater than 0.2 are sex, age, address, famsize, Medu, Fedu, Mjob, Fjob, traveltime, studytime, failures, schoolsup, paid, higher, romantic, goout, G1 and G2. They are more correlated with G3 compared to other variables. Whereas school, Pstatus, reason, guardian, famsup, activities, nursery, internet, famrel, freetime, absences, Dalc, Walc and health variables are less related with G3 compared to other variables. We will exclude G1 and G2 from now on. Because we want to know the reason behind poor results except the previous result of any performance. 
- Second, indirect influential variables whose absolute value of the correlation with the direct influential variables is greater than 0.2 are famsup, school, internet, Dalc, Walc, freetime and gauardian variables. They are more correlated with direct influential variables correlated with G3 directly compared to other variables. 
- Third, excluded variables are Pstatus, reason, famsup, activities, nursery, famrel, health and absences. This is because they seem less important variables in consideration with correlation with G3. 

#### l. Create a dataset which is excluded less related variables
```{r}
imp_num<-ex_num[, !(names(ex_num)%in% c("Pstatus", "reason", "famsup", "activities", "nursery", "famrel", "health", "absences"))]
```

#### m. Show correlation table and graph at a time
```{r}
#install.packages("PerformanceAnalytics")
chart.Correlation(num, Histogram=T)
```

Although absences variable has shown in the above graph that it has * correlated with age, Medu, Dalc Walc variables, we will still exclude it because it is less than 0.2

#### n. Show analysis of relation among decisive variables on G3
The most three direct decisive variables are failure, Medu and studytime. And many variables are related with these decisive variables. Among them, We will examine the following. 

- failure, age and sex(gender)

The reason to choose it is that failure is the most decisive variable on G3. Age has correlation number with failure (0.24) and worth exploring. Sex(gender) has correlation with studytime(0.31) which has correlation with failure.

#### n-1. Show G1+G2+G3, age and sex relation

```{r}
G1=ggplot(data=mat,aes(x=age, y=G1+G2+G3, col=sex, shape=sex))+geom_point()+geom_smooth(method="lm",se=F)+facet_grid(~sex)
G1
```

The girls' grades are consistent with their age whereas boys' grades are getting decreased with their age. Behind of the results might be explained with the following questions and codes. 

#### n-2.Why boys are getting bad at their grades with their age? 

```{r}
G1=ggplot(data=mat,aes(x=age, y=failures, col=sex, shape=sex))+geom_point()+geom_smooth(method="lm",se=F)+facet_grid(~sex)
G1
```

As the students are getting older, they might experience failures which directly affect the results of grades. Thy boys might experience more failures than girls. Let's deep into the reason. 

#### n-3. Which variable affect boys' failure? 
Let's explore alcohol consumption because one of variables that related with failures are studytime which is also related Dalc and Walc. 

```{r}
my_graph <- ggplot(mat, aes(x = Dalc + Walc, y = G1+G2+G3)) +
    geom_point(aes(color = sex)) +
    stat_smooth(method = "lm",
    se = FALSE,
    size = 3)
my_graph
```

As the line explains that as alcohol consumption increases the grades decrease. Alcohol intake may lead to frequent confusion and an inability to remember, which results in poor performance. The higher amount of alcohol consumption is from boys. Boys's higher intake of alcohol than girls might affect on their poorer grades than girls.  

#### n-4. Might a negatively correlated variable, traveltime affect on male's relatively poor results?
```{r}
table(mat$traveltime)
travel=ggplot(data=mat, aes(x=traveltime, y=G1, col=sex))+geom_point()+geom_smooth(method="lm",se=F)+facet_grid(~sex)
travel
```

As shown above, the negative impact of the traveltime could be largely seen in the boys' performance, the further the male student resides from the school, the less their performance would they get.

Therefore, the difference between results of female and male would be explained with failures, alcohol consumption, traveltime. 

### 2.3.  Data Pre-processing_Data Modification and Cleaning 

#### 2.3.1. Checking NA (Null Values)
We have already checked the missing values in several statistic tables before. Let's one more check using a simple code. 
```{r}
# Check missing data
sapply(mat, anyNA)
```
There is no NA in all columns. 

#### 2.3.2. Checking Outliers
```{r}
boxplot(num, main="Multiplot Visualization for numeric variables" )
```

Absences variable has many outliers.

#### 2.3.3. Data Partition
Let's split dataset into trainset and testset.

```{r}
set.seed(3, sample.kind = "Rounding") 

partition <- createDataPartition(new_mat[,'G3'], times=1, p=0.80, list=FALSE)
new_mat_trainset<-new_mat[partition,]
new_mat_testset<-new_mat[-partition,]

partition <- createDataPartition(ex_new_mat[,'G3'], times=1, p=0.80, list=FALSE)
ex_new_mat_trainset<-ex_new_mat[partition,]
ex_new_mat_testset<-ex_new_mat[-partition,]

partition <- createDataPartition(ex_bi[,'G3'], times=1, p=0.80, list=FALSE)
ex_bi_trainset<-ex_bi[partition,]
ex_bi_testset<-ex_bi[-partition,]
```

## 3. Modelling
In this section, the process and technique are the following.

a. Process and Techniques <br> We will go through 4 steps:
  - Training models with train datasets
  - Predicting test datasets
  - Evaluating models using accuracy instrument
  - Showing accumulated results tables
  
b. Models <br>
We will apply several machine learning algorithms and build following models to predict whether students would pass or fail the final grade G3.<br>
  - Model 1: Guessing Model
  - Model 2: Logistic regression Model
  - Model 3: Simplified Logistic Regression with Significant Variables
  - Model 4: Cross-validated Decision Tree
  - Model 5: Cross-validated KNN Model
  - Model 6: Random Forest Model
  
### 3.1. Model 1: Baseline prediction by randomly guessing the outcome
The simplest prediction method is randomly guessing the outcome, whether that person passed or not by sampling from the vector c(0,1), without using additional predictors. These methods will help us determine whether our machine learning algorithm performs better than chance. How accurate are this method of guessing students' final test?

Let's apply this method to different datasets.

#### Modeling and Prediction
```{r}
# set.seed(3)  
set.seed(1, sample.kind = "Rounding")

# Guess with equal probability of pass
guess <- sample(c(0,1), nrow(new_mat_trainset), replace = TRUE)
mean(guess == new_mat_testset$G3)
```

```{r}
# set.seed(3)  
set.seed(2, sample.kind = "Rounding")

# guess with equal probability of pass with ex_new_mat dataset
guess <- sample(c(0,1), nrow(ex_new_mat_trainset), replace = TRUE)
mean(guess == ex_new_mat_testset$G3)
```

```{r}
# set.seed(3)  
set.seed(3, sample.kind = "Rounding")

# guess with equal probability of pass with ex_bi dataset
guess <- sample(c(0,1), nrow(ex_bi_trainset), replace = TRUE)
guess_results<-mean(guess == ex_bi_testset$G3)
guess_results
```

The best result of guessing method was when applying to binary dataset, ex_bi.

#### Result Table
```{r}
results <- tibble(Method = "Model 1: Guessing Model", 
                            Accuracy = guess_results)
results %>% knitr::kable()
```

### 3.2. Model 2: Logistic Regression Model 

#### Modeling
```{r}
# Regression_glm
set.seed(3, sample.kind = "Rounding") # set.seed function is to ensure that the samples produced are reproducible
ex_new_mat_log_model<-glm(G3~., data=ex_new_mat_trainset, family=binomial)
ex_new_mat_log_model
```

```{r}
summary(ex_new_mat_log_model)
```

#### Prediction
```{r}
predict_log1<-predict(ex_new_mat_log_model, newdata=ex_new_mat_testset, type="response")
predict_log1<-round(predict_log1)
log_results<-mean(predict_log1==ex_new_mat_testset$G3)
log_results
```
#### Result Table
```{r}
results <- bind_rows(results, 
                     tibble(Method = "Model 2: Logistic Regression Model", 
                            Accuracy = log_results))
results %>% knitr::kable()
```

We used the logistic regression algorithm to predict whether the students would pass or not. The output of this model are probabilities to happen with comparison with an unbiased threshold of 0.5. We see that around 82% of results were accurately predicted despite of the fact that we has removed decisive variables such as G1, G2. 

According to the summary of the model, the significant predictors are failures, sex, paid, goout, Walc, goout, studytime and absences. 

### 3.3. Model 3: Simplified Logistic Regression Model with Significant Variables

#### Modeling
Correlation EDA shows that significant variables are sex, age, address, famsize, Medu, Fedu, Mjob, Fjob, traveltime, studytime, failures, schoolsup, paid, higher, romantic and goout. Let's simplify variable and apply to the regression model. 
```{r}
set.seed(3, sample.kind = "Rounding") # set.seed function is to ensure that the samples produced are reproducible
sig_log_model<-glm(G3~failures + sex + address + age + Medu + Fedu+ Mjob + Fjob + schoolsup + paid + higher + romantic + goout + studytime  + traveltime + famsize, data=ex_new_mat_trainset, family=binomial)
sig_log_model
```

```{r}
summary(sig_log_model)
```

#### Prediction
```{r}
predict_log2<-predict(sig_log_model, newdata=ex_new_mat_testset, type="response")
predict_log2<-round(predict_log2)
sig_log_results<-mean(predict_log2==ex_new_mat_testset$G3)
sig_log_results
```
Accuraccy has been improved from around 0.82 to 0.84. 

#### Result Table
```{r}
results <- bind_rows(results, 
                     tibble(Method = "Model 3: Simplified Logistic Regression Model with Significant Variables", 
                            Accuracy = sig_log_results))
results %>% knitr::kable()
```


### 3.4. Model 4: Decision Tree
The Decision Tree(DT) looks like upside down trees where the root of the tree is on top and one goes down step by step towards the leaves. Its internal rule looks like IF-THEN rules in r. Each internal node represents a “test” on an attribute. Each branch represents the outcome of the test and each leaf node represents a class label. 

#### Visualization
```{r}
dtree<-rpart(G3~., data=ex_bi_trainset)
fancyRpartPlot(dtree, main="Model 4: Decision Tree", palettes=c("Blues", "Reds"))
```

#### Modeling
```{r}
# Cross validation of the model. 
# Let's do? cross validation of the model to assess how the results of a statistical analysis will generalize with the optimal cp.

set.seed(3, sample.kind = "Rounding")  
tc <- trainControl(method = "cv",
                   number = 10)
cp_grid <- expand.grid(cp = seq(0, 0.03, 0.001))


dtree_cv <- train(G3~., 
                      na.action=na.omit,
                      data = ex_bi_trainset, 
                      method = "rpart", 
                      trControl = tc,
                      tuneGrid = cp_grid)
dtree_cv
```
#### Prediction
```{r}
# Test the model in the fnew_mat_testset with the optimal value for the model, cp=0.027.
set.seed(3, sample.kind = "Rounding")
cp_grid <- expand.grid(cp = 0.027)

dtree_cv <- train(G3~.,
                      data = ex_bi_testset, 
                      na.action=na.omit,
                      method = "rpart", 
                      trControl = tc,
                      tuneGrid = cp_grid)
dtree_cv
```
#### Result

```{r}
# What is the accuracy on the test set using the cross-validated DT model?
dt_pred <- predict(dtree_cv, ex_bi_testset)
dt_results<-mean(dt_pred == ex_bi_testset$G3) # accuracy of the KNN model on the test set
dt_results
```
#### Result Table
```{r}
results <- bind_rows(results, 
                     tibble(Method = "Model 4: Cross-validated Decison Tree Model", 
                            Accuracy = dt_results))
results %>% knitr::kable()
```
### 3.5. Model 5: KNN (K-Nearest Neighbors)
Every student is stored. A new student is comapared to the stored students. Let's find the k most similar students. The most common grade of the k studetns is given to a new student. 

#### Modeling

```{r}
# Let's use caret to train a decision tree with the rpart method.
# The caret package performs cross validation for us and lets us train different algorithms using similart syntax. 

# Predict
#set.seed(6)
set.seed(6, sample.kind = "Rounding") # if using R 3.6 or later
train_knn_cv <- train(G3 ~ .,
                      method = "knn",
                      data = ex_bi_trainset, na.action=na.omit,
                      tuneGrid = data.frame(k = seq(3, 51, 2)),# Try tuning with k=seq(3,51,2).
                      trControl = trainControl(method = "cv", number = 10, p = 0.9))# 10-fold cross validation where each partition consists of 10% of the total. 

# Optimal value of k using cross-validation
train_knn_cv$bestTune # parameter that maximized the estimated accuracy
```
The optimal value of k is 35. 

```{r}
print(train_knn_cv)
```
#### Results

```{r}
# What is the accuracy on the test set using the cross-validated KNN model?
# Accuracy
knn_cv_pred <- predict(train_knn_cv, ex_bi_testset)
# mean(knn_cv_pred == fnew_mat_test_set$G3)
knn_cv_results<-mean(knn_cv_pred== ex_bi_testset$G3) # accuracy of the KNN model on the test set
knn_cv_results
```
#### Visualization
```{r}
ggplot(train_knn_cv)
```

#### Results Table

```{r}
results <- bind_rows(results, 
                     tibble(Method = "Model 5: Cross-validated KNN Model", 
                            Accuracy = knn_cv_results))
results %>% knitr::kable()
```

### 3.6. Model 6. Random Forest Model

The Random Forest (RF) (Breiman 2001) is an ensemble unpruned Decision Tree(DT). Each tree is based on a random feature selection from bootstrap training samples and the RF predictions are built by averaging the outputs of the trees. 

#### Modeling
```{r}
# Set the seed to 14. Use the caret train() function with the rf method to train a random forest. Test values of mtry = seq(1:7). Set ntree to 100.
# What mtry value maximizes accuracy?

#set.seed(14)  
set.seed(14, sample.kind = "Rounding")    # simulate R 3.5
train_rf <- train(G3 ~ .,
                  data = ex_bi_trainset,
                  na.action=na.omit,
                  method = "rf",
                  ntree = 100,
                  tuneGrid = data.frame(mtry = seq(1:7)))
train_rf$bestTune
```

#### Visualization
```{r}
# What is the best value for mtry? Lets try to find the optimal value for mtry parameter (Number of variables randomly sampled as candidates at each split).
plot(train_rf)
```
From the graph above and from the results, we can conclude that optimal number of variables is 6

```{r}
print(train_rf)
```
#### Result
```{r}
# What is the accuracy of the random forest model on the test set? 
rf_preds <- predict(train_rf, ex_bi_testset) 
rf_results<-mean(rf_preds==ex_bi_testset$G3)
rf_results
```
#### Results Table
```{r}
results <- bind_rows(results, 
                     tibble(Method = "Model 6: Random Forest Model", 
                            Accuracy = rf_results))
results %>% knitr::kable()
```

```{r} 
# What is the most important variable? 
varImp(train_rf)  
```
Some important variables in the above table are failures, paid and goout as the same as correlation graphs but we can see slightly different result. For example, we excluded absences because of relatively smaller correlation coefficient but we can see absences variable is one of the important variable in rf model. The most striking feature is romantic is the most important variable in rf model unlike correlation table.  

## 4. Results
Now, let's see which method has performed the best among guessing, Regression, DT, KNN and RF.
```{r}
# Show the final result
results 
```

The best model is Simplified Logistic Regression Model with Significant Variables with accuracy of around 0.84. 

## 5. Conclusion
#### Summary
The goal of this paper was to predict students' final performance result of either pass or fail at the thought that educators might help students who has higher probability to fail in advance. To achieve the goal, the aim of this paper was to build a good model with high accuracy result for binary classification data and continuous data to approach some machine learning algorithms. During EDA, we had attempted to get influential predictors with which we wanted to build model such as Regreesion.

We collected a data named "student-mat" and get information from UCI website. We overviewed of the dataset and analyzed relation among variables. From data analysis we could find out important and influential variables that affect the result of the dependent variable. They  are sex, age, address, famsize, Medu, Fedu, Mjob, Fjob, traveltime, studytime, failures, schoolsup, paid, higher, romantic, goout, G1 and G2. They are correlated with G3 compared to other variables. Indirect influential variables are famsup, school, internet, Dalc, Walc, freetime and gauardian variables. They are correlated with the influential variables that is correlated with G3 directly. Excluded variables are Pstatus, reason, famsup, activities, nursery, famrel, health and absences variables that are less correlated with G3 directly and indirectly. 

Before builing up several algorithms, we modified our dataset by checking NA and outliers and by splitting into trainset and testset. We approached some algorithms such as Guessing, Regression, Decision Trees, KNN and Random Forest method training each model with trainset and predicting with testset. The models were evaluated by accuracy instrument and the best optimal model was selected: Simplified Logistic Regression Model with Significant Variables with accuracy of around 0.84.   

#### Potential Impact
This paper may have potential impact in that we get some implied information in education field from the raw data, which would make educators to approach presumable week students and give more attention. This might make contribution to achieve equity in education domain. This model can be used for predicting students with a fairly decent accuracy. 

#### Limitation
We modified dataset into several types and applied in accordance with a model that requires specific type of data. Despite of the fact of using different type of dataset, the content remained the same meaning. We just wanted to find out the best model. However, it would be more useful to compare if we use the same data type. In addition, we may apply to other algorithms that we don't know yet. Further study is needed for a better promising model. 

#### Further work: 
To get a better result, we may add more data and apply other unexplored machine learning methods. In addition, more research is need to study on consideration on how educators can supplement the decisive and deficit factors on their grades. Also, real data related either Singapore or Korea is needed in order to apply in practice. Further, research on online learning environment would be potential especially in the pandemic situation like Covid-19.  

## Reference
P. Cortez and A. Silva. Using Data Mining to Predict Secondary School Student Performance. In A. Brito and J. Teixeira Eds., Proceedings of 5th FUture BUsiness TEChnology Conference (FUBUTEC 2008) pp. 5-12, Porto, Portugal, April, 2008, EUROSIS, ISBN 978-9077381-39-7.


